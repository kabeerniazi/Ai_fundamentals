{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practical Lab: Understanding Feature Scaling and Optimizing Learning Rates\n",
    "\n",
    "When working with multiple features in machine learning, two critical factors affect training performance: the learning rate and feature scaling. This lab explores these concepts hands-on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Objectives\n",
    "\n",
    "By completing this lab, you will:\n",
    "- Apply gradient descent to datasets with multiple input features\n",
    "- Experiment with different learning rates and observe their effects on convergence\n",
    "- Understand why feature scaling matters for optimization\n",
    "- Implement z-score normalization to improve training efficiency\n",
    "- Build on the multi-variable regression functions from previous work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Dependencies\n",
    "\n",
    "We'll use NumPy for numerical computations and matplotlib for visualization. The lab also includes helper functions for loading data and running gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from lab_utils_multi import  load_house_data, run_gradient_descent \n",
    "from lab_utils_multi import  norm_plot, plt_equal_scale, plot_cost_i_w\n",
    "from lab_utils_common import dlc\n",
    "np.set_printoptions(precision=2)\n",
    "plt.style.use('./deeplearning.mplstyle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mathematical Notation Reference\n",
    "\n",
    "Throughout this notebook, we use the following notation:\n",
    "\n",
    "|Symbol | Meaning | Python Variable |\n",
    "|:------|:--------|:----------------|\n",
    "| $a$ | scalar value (not bold) | |\n",
    "| $\\mathbf{a}$ | vector (bold lowercase) | |\n",
    "| $\\mathbf{A}$ | matrix (bold uppercase) | |\n",
    "| **For Regression Problems:** | | |\n",
    "|  $\\mathbf{X}$ | matrix of training examples | `X_train` |   \n",
    "|  $\\mathbf{y}$  | vector of target values | `y_train` |\n",
    "|  $\\mathbf{x}^{(i)}$, $y^{(i)}$ | $i^{th}$ training example and its target | `X[i]`, `y[i]`|\n",
    "| $m$ | total number of training examples | `m`|\n",
    "| $n$ | number of input features | `n`|\n",
    "|  $\\mathbf{w}$  |  weight parameters (one per feature) | `w` |\n",
    "|  $b$ |  bias parameter | `b` |     \n",
    "| $f_{\\mathbf{w},b}(\\mathbf{x}^{(i)})$ | Model prediction: $f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) = \\mathbf{w} \\cdot \\mathbf{x}^{(i)}+b$ | `f_wb` | \n",
    "|$\\frac{\\partial J(\\mathbf{w},b)}{\\partial w_j}$| Partial derivative of cost with respect to weight $w_j$ |`dj_dw[j]`| \n",
    "|$\\frac{\\partial J(\\mathbf{w},b)}{\\partial b}$| Partial derivative of cost with respect to bias $b$| `dj_db`|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Housing Price Prediction Challenge\n",
    "\n",
    "Let's tackle a practical problem: predicting house prices based on multiple characteristics. Our dataset includes houses with four features:\n",
    "- Size in square feet\n",
    "- Number of bedrooms\n",
    "- Number of floors\n",
    "- Age in years\n",
    "\n",
    "Our goal is to build a linear regression model that can predict the price of any house. For instance, if someone asks \"What's a fair price for a 1200 sqft house with 3 bedrooms, 1 floor, and 40 years old?\", our model should provide an estimate.\n",
    "\n",
    "Note: Unlike earlier exercises that used 1000s of sqft, this dataset uses actual square footage.\n",
    "\n",
    "## Sample Data\n",
    "\n",
    "Here's a glimpse of our training data:\n",
    "\n",
    "| Size (sqft) | Bedrooms  | Floors | Age (years) | Price ($1000s) |   \n",
    "| ----------- | --------- | ------ | ----------- | -------------- |  \n",
    "| 952         | 2         | 1      | 65          | 271.5          |  \n",
    "| 1244        | 3         | 2      | 64          | 232            |  \n",
    "| 1947        | 3         | 2      | 17          | 509.8          |  \n",
    "| ...         | ...       | ...    | ...         | ...            |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the dataset\n",
    "X_train, y_train = load_house_data()\n",
    "X_features = ['size(sqft)','bedrooms','floors','age']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize our data to understand which features might be most predictive of price."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax=plt.subplots(1, 4, figsize=(12, 3), sharey=True)\n",
    "for i in range(len(ax)):\n",
    "    ax[i].scatter(X_train[:,i],y_train)\n",
    "    ax[i].set_xlabel(X_features[i])\n",
    "ax[0].set_ylabel(\"Price (1000's)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the scatter plots above, we can see different patterns:\n",
    "- **Size**: Clear positive correlation - larger homes cost more\n",
    "- **Bedrooms & Floors**: Weaker relationship with price\n",
    "- **Age**: Newer homes (lower age values) tend to have higher prices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"toc_15456_5\"></a>\n",
    "## Multi-Feature Gradient Descent\n",
    "\n",
    "Gradient descent for multiple variables follows this update rule (iterating until convergence):\n",
    "\n",
    "$$\\begin{align*} \\text{repeat}&\\text{ until convergence:} \\; \\lbrace \\newline\\;\n",
    "& w_j := w_j -  \\alpha \\frac{\\partial J(\\mathbf{w},b)}{\\partial w_j} \\tag{1}  \\; & \\text{for j = 0..n-1}\\newline\n",
    "&b\\ \\ := b -  \\alpha \\frac{\\partial J(\\mathbf{w},b)}{\\partial b}  \\newline \\rbrace\n",
    "\\end{align*}$$\n",
    "\n",
    "where $n$ represents the feature count, and all parameters ($w_j$ and $b$) update simultaneously.\n",
    "\n",
    "The partial derivatives are computed as:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial J(\\mathbf{w},b)}{\\partial w_j}  &= \\frac{1}{m} \\sum\\limits_{i = 0}^{m-1} (f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) - y^{(i)})x_{j}^{(i)} \\tag{2}  \\\\\n",
    "\\frac{\\partial J(\\mathbf{w},b)}{\\partial b}  &= \\frac{1}{m} \\sum\\limits_{i = 0}^{m-1} (f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) - y^{(i)}) \\tag{3}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "* $m$ is the number of training examples\n",
    "    \n",
    "*  $f_{\\mathbf{w},b}(\\mathbf{x}^{(i)})$ represents our model's prediction, while $y^{(i)}$ is the actual target\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Learning Rate: Finding the Sweet Spot\n",
    "<figure>\n",
    "    <img src=\"./images/C1_W2_Lab06_learningrate.PNG\" style=\"width:1200px;\" >\n",
    "</figure>\n",
    "\n",
    "The learning rate $\\alpha$ is a hyperparameter that controls how big a step we take during each iteration of gradient descent. It's shared across all parameters and can dramatically affect training:\n",
    "\n",
    "- **Too large**: The algorithm overshoots and may diverge\n",
    "- **Too small**: Training is slow but stable\n",
    "- **Just right**: Efficient convergence to the optimal solution\n",
    "\n",
    "Let's experiment with different learning rates on our housing dataset to see these effects in action."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 1: $\\alpha$ = 9.9e-7 (Too High)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set alpha to 9.9e-7\n",
    "_, _, hist = run_gradient_descent(X_train, y_train, 10, alpha = 9.9e-7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the cost is *increasing* instead of decreasing! This is a clear sign that our learning rate is too large - the algorithm is overshooting the minimum and diverging rather than converging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_cost_i_w(X_train, y_train, hist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The visualization shows what's happening: the parameter $w_0$ (right plot) oscillates wildly, overshooting the optimal value with each iteration. This causes the cost (left plot) to increase rather than decrease toward the minimum.\n",
    "\n",
    "Note: While all 4 parameters are being updated simultaneously, this visualization focuses on $w_0$ to illustrate the concept. The blue and orange lines may appear slightly offset due to this simplification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Experiment 2: $\\alpha$ = 9e-7 (Better)\n",
    "\n",
    "Let's reduce the learning rate slightly and observe the improvement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set alpha to 9e-7\n",
    "_,_,hist = run_gradient_descent(X_train, y_train, 10, alpha = 9e-7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Good news! The cost is now consistently decreasing throughout the training, indicating our learning rate is in a workable range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_cost_i_w(X_train, y_train, hist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The left plot confirms cost is decreasing properly. The right plot shows $w_0$ still oscillates around the optimal value, but now the cost decreases with each step rather than increasing. Notice how `dj_dw[0]` alternates sign as `w[0]` jumps across the minimum.\n",
    "\n",
    "This learning rate will eventually converge, though not optimally. Try adjusting the iteration count to see the full convergence behavior."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 3: $\\alpha$ = 1e-7 (Optimal)\n",
    "\n",
    "What happens if we reduce the learning rate even further?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set alpha to 1e-7\n",
    "_,_,hist = run_gradient_descent(X_train, y_train, 10, alpha = 1e-7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Excellent! The cost continues to decrease steadily, confirming $\\alpha$ is in a safe range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_cost_i_w(X_train,y_train,hist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perfect behavior! The left plot shows steady cost reduction. The right plot reveals that $w_0$ smoothly approaches the minimum without oscillation - notice how `dj_w0` stays negative throughout, indicating consistent progress in the right direction. This learning rate will converge reliably."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Why Feature Scaling Matters\n",
    "<figure>\n",
    "    <img src=\"./images/C1_W2_Lab06_featurescalingheader.PNG\" style=\"width:1200px;\" >\n",
    "</figure>\n",
    "\n",
    "Our experiments revealed that even with a well-chosen learning rate, convergence is sluggish. Why? The answer lies in feature scaling.\n",
    "\n",
    "### The Problem with Unscaled Features\n",
    "\n",
    "Look at our housing data:\n",
    "- Size ranges from ~900 to 2000 sqft\n",
    "- Number of bedrooms ranges from 1 to 5\n",
    "\n",
    "These vastly different scales cause gradient descent to take inefficient, zig-zagging steps. Small changes in the \"size\" parameter have huge effects on predictions, while large changes in \"bedrooms\" barely matter.\n",
    "\n",
    "### The Solution: Z-Score Normalization\n",
    "\n",
    "Z-score normalization (also called standardization) transforms each feature to have:\n",
    "- Mean ($\\mu$) = 0\n",
    "- Standard deviation ($\\sigma$) = 1\n",
    "\n",
    "The formula:\n",
    "$$x^{(i)}_j = \\frac{x^{(i)}_j - \\mu_j}{\\sigma_j}$$\n",
    "\n",
    "where:\n",
    "- $\\mu_j$ is the mean of feature $j$\n",
    "- $\\sigma_j$ is the standard deviation of feature $j$\n",
    "\n",
    "After normalization, all features are on a comparable scale, allowing gradient descent to converge much faster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>\n",
    "    <font size='3', color='darkgreen'><b>Technical Deep-Dive: Why Feature Scaling Works</b></font>\n",
    "</summary>\n",
    "\n",
    "Let's examine what happens during training with $\\alpha$ = 9e-7 more closely:\n",
    "\n",
    "<figure>\n",
    "    <img src=\"./images/C1_W2_Lab06_ShortRun.PNG\" style=\"width:1200px;\" >\n",
    "</figure>\n",
    "\n",
    "In the early iterations above, notice that $w_0$ (corresponding to size) changes dramatically while other parameters barely budge. This is because $w_0$ has a much larger gradient.\n",
    "\n",
    "Here's what a very long training run looks like (this can take hours!):\n",
    "\n",
    "<figure>\n",
    "    <img src=\"./images/C1_W2_Lab06_LongRun.PNG\" style=\"width:1200px;\" >\n",
    "</figure>\n",
    "    \n",
    "Observe how cost decreases rapidly at first, then crawls. $w_0$ quickly reaches its optimal value (notice `dj_dw0` becomes tiny), while $w_1$, $w_2$, and $w_3$ take much longer to converge.\n",
    "\n",
    "**Why does this happen?**\n",
    "\n",
    "<figure>\n",
    "    <center> <img src=\"./images/C1_W2_Lab06_scale.PNG\"   ></center>\n",
    "</figure>   \n",
    "\n",
    "The gradient descent update rule multiplies the error by each feature value:\n",
    "- $\\alpha$ is the same for all parameters\n",
    "- The error term is shared across all updates\n",
    "- But each $w_j$ gets multiplied by feature $x_j$\n",
    "\n",
    "Since house size is typically 1000+ while bedroom count is 2-4, $w_0$'s gradient is hundreds of times larger than $w_1$'s gradient. This creates uneven updates.\n",
    "\n",
    "Feature scaling solves this by normalizing all features to similar ranges.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalization Techniques\n",
    "\n",
    "Several methods exist for feature normalization:\n",
    "\n",
    "1. **Min-Max Scaling**: Rescales features to [0, 1] or [-1, 1] range\n",
    "   - Formula: $x_i := \\dfrac{x_i - min}{max - min}$\n",
    "\n",
    "2. **Mean Normalization**: Centers data around zero\n",
    "   - Formula: $x_i := \\dfrac{x_i - \\mu_i}{max - min}$\n",
    "\n",
    "3. **Z-Score Normalization** (what we'll use): Standardizes to mean=0, std=1\n",
    "\n",
    "We'll focus on z-score normalization as it's particularly effective for gradient descent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Implementing Z-Score Normalization\n",
    "\n",
    "Z-score normalization transforms features so they all have mean = 0 and standard deviation = 1.\n",
    "\n",
    "The transformation formula:\n",
    "$$x^{(i)}_j = \\dfrac{x^{(i)}_j - \\mu_j}{\\sigma_j} \\tag{4}$$ \n",
    "\n",
    "where:\n",
    "- $j$ identifies which feature (column) in $\\mathbf{X}$\n",
    "- $\\mu_j$ is the mean of feature $j$ across all examples\n",
    "- $\\sigma_j$ is the standard deviation of feature $j$\n",
    "\n",
    "Computing the statistics:\n",
    "$$\n",
    "\\begin{align}\n",
    "\\mu_j &= \\frac{1}{m} \\sum_{i=0}^{m-1} x^{(i)}_j \\tag{5}\\\\\n",
    "\\sigma^2_j &= \\frac{1}{m} \\sum_{i=0}^{m-1} (x^{(i)}_j - \\mu_j)^2  \\tag{6}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    ">**Critical Implementation Note:** Save the mean and standard deviation values you compute during training! When making predictions on new data, you must normalize using these same statistics (not new ones calculated from the test data). Otherwise, your model's predictions will be incorrect.\n",
    "\n",
    "Let's implement this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zscore_normalize_features(X):\n",
    "    \"\"\"\n",
    "    computes  X, zcore normalized by column\n",
    "    \n",
    "    Args:\n",
    "      X (ndarray (m,n))     : input data, m examples, n features\n",
    "      \n",
    "    Returns:\n",
    "      X_norm (ndarray (m,n)): input normalized by column\n",
    "      mu (ndarray (n,))     : mean of each feature\n",
    "      sigma (ndarray (n,))  : standard deviation of each feature\n",
    "    \"\"\"\n",
    "    # find the mean of each column/feature\n",
    "    mu     = np.mean(X, axis=0)                 # mu will have shape (n,)\n",
    "    # find the standard deviation of each column/feature\n",
    "    sigma  = np.std(X, axis=0)                  # sigma will have shape (n,)\n",
    "    # element-wise, subtract mu for that column from each example, divide by std for that column\n",
    "    X_norm = (X - mu) / sigma      \n",
    "\n",
    "    return (X_norm, mu, sigma)\n",
    " \n",
    "#check our work\n",
    "#from sklearn.preprocessing import scale\n",
    "#scale(X_orig, axis=0, with_mean=True, with_std=True, copy=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize what z-score normalization does to our data step-by-step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu     = np.mean(X_train,axis=0)   \n",
    "sigma  = np.std(X_train,axis=0) \n",
    "X_mean = (X_train - mu)\n",
    "X_norm = (X_train - mu)/sigma      \n",
    "\n",
    "fig,ax=plt.subplots(1, 3, figsize=(12, 3))\n",
    "ax[0].scatter(X_train[:,0], X_train[:,3])\n",
    "ax[0].set_xlabel(X_features[0]); ax[0].set_ylabel(X_features[3]);\n",
    "ax[0].set_title(\"unnormalized\")\n",
    "ax[0].axis('equal')\n",
    "\n",
    "ax[1].scatter(X_mean[:,0], X_mean[:,3])\n",
    "ax[1].set_xlabel(X_features[0]); ax[0].set_ylabel(X_features[3]);\n",
    "ax[1].set_title(r\"X - $\\mu$\")\n",
    "ax[1].axis('equal')\n",
    "\n",
    "ax[2].scatter(X_norm[:,0], X_norm[:,3])\n",
    "ax[2].set_xlabel(X_features[0]); ax[0].set_ylabel(X_features[3]);\n",
    "ax[2].set_title(r\"Z-score normalized\")\n",
    "ax[2].axis('equal')\n",
    "plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "fig.suptitle(\"distribution of features before, during, after normalization\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The visualization above shows how normalization transforms two features (\"size\" and \"age\") with equal axis scaling:\n",
    "\n",
    "- **Left (Original)**: Notice how \"size(sqft)\" has a much wider range than \"age\". The data is stretched horizontally.\n",
    "- **Middle (After centering)**: Subtracting the mean ($\\mu$) shifts both features to center around zero. \"Size\" is now clearly centered at zero, though \"age\" is harder to see due to scale.\n",
    "- **Right (After full normalization)**: Dividing by standard deviation ($\\sigma$) brings both features to the same scale. Now they're both centered at zero with comparable variance.\n",
    "\n",
    "This transformation makes gradient descent much more efficient!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's apply normalization to our entire dataset and examine the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize the original features\n",
    "X_norm, X_mu, X_sigma = zscore_normalize_features(X_train)\n",
    "print(f\"X_mu = {X_mu}, \\nX_sigma = {X_sigma}\")\n",
    "print(f\"Peak to Peak range by column in Raw        X:{np.ptp(X_train,axis=0)}\")   \n",
    "print(f\"Peak to Peak range by column in Normalized X:{np.ptp(X_norm,axis=0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice the dramatic improvement! Normalization reduced the peak-to-peak range from thousands down to just 2-3 for each feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax=plt.subplots(1, 4, figsize=(12, 3))\n",
    "for i in range(len(ax)):\n",
    "    norm_plot(ax[i],X_train[:,i],)\n",
    "    ax[i].set_xlabel(X_features[i])\n",
    "ax[0].set_ylabel(\"count\");\n",
    "fig.suptitle(\"distribution of features before normalization\")\n",
    "plt.show()\n",
    "fig,ax=plt.subplots(1,4,figsize=(12,3))\n",
    "for i in range(len(ax)):\n",
    "    norm_plot(ax[i],X_norm[:,i],)\n",
    "    ax[i].set_xlabel(X_features[i])\n",
    "ax[0].set_ylabel(\"count\"); \n",
    "fig.suptitle(\"distribution of features after normalization\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The histograms above show a key insight: after normalization, all features share similar x-axis ranges (roughly -2 to +2) and are centered around zero. This uniform scaling is exactly what gradient descent needs for efficient convergence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time to test gradient descent on our normalized data!\n",
    "\n",
    "Pay attention to the **much larger learning rate** we can now use (0.1 vs 1e-7). This dramatically accelerates training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_norm, b_norm, hist = run_gradient_descent(X_norm, y_train, 1000, 1.0e-1, )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Impressive results! With normalized features, we achieved excellent accuracy in just 1000 iterations - **orders of magnitude faster** than before. The tiny gradients at the end confirm convergence.\n",
    "\n",
    "A learning rate of 0.1 works great for normalized features. Let's visualize how well our model predicts by comparing predictions to actual prices. (Note: predictions use normalized features, but the plot shows original feature values for interpretability.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predict target using normalized features\n",
    "m = X_norm.shape[0]\n",
    "yp = np.zeros(m)\n",
    "for i in range(m):\n",
    "    yp[i] = np.dot(X_norm[i], w_norm) + b_norm\n",
    "\n",
    "    # plot predictions and targets versus original features    \n",
    "fig,ax=plt.subplots(1,4,figsize=(12, 3),sharey=True)\n",
    "for i in range(len(ax)):\n",
    "    ax[i].scatter(X_train[:,i],y_train, label = 'target')\n",
    "    ax[i].set_xlabel(X_features[i])\n",
    "    ax[i].scatter(X_train[:,i],yp,color=dlc[\"dlorange\"], label = 'predict')\n",
    "ax[0].set_ylabel(\"Price\"); ax[0].legend();\n",
    "fig.suptitle(\"target versus prediction using z-score normalized model\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The predictions look excellent! A few observations:\n",
    "- With multiple features, we can't visualize everything in a single plot - hence the four separate plots.\n",
    "- Remember: these predictions use normalized features. Any new data must be normalized using the **same** mean and standard deviation from training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making Predictions on New Data\n",
    "\n",
    "The whole purpose of building this model is to predict prices for houses not in our dataset. Let's predict the price of a house with:\n",
    "- 1200 sqft\n",
    "- 3 bedrooms\n",
    "- 1 floor\n",
    "- 40 years old\n",
    "\n",
    "**Critical reminder:** You must normalize this new data using the same $\\mu$ and $\\sigma$ computed from the training set!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, normalize out example.\n",
    "x_house = np.array([1200, 3, 1, 40])\n",
    "x_house_norm = (x_house - X_mu) / X_sigma\n",
    "print(x_house_norm)\n",
    "x_house_predict = np.dot(x_house_norm, w_norm) + b_norm\n",
    "print(f\" predicted price of a house with 1200 sqft, 3 bedrooms, 1 floor, 40 years old = ${x_house_predict*1000:0.0f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing the Cost Function Landscape\n",
    "\n",
    "<img align=\"left\" src=\"./images/C1_W2_Lab06_contours.PNG\"   style=\"width:240px;\" >\n",
    "\n",
    "Cost contour plots provide another perspective on why feature scaling matters. When features have mismatched scales, the cost function becomes highly asymmetric.\n",
    "\n",
    "The plots below compare cost contours before and after normalization:\n",
    "- **Left**: Before normalization - the contour plot of $w_0$ (size) vs $w_1$ (bedrooms) is so stretched that you can barely see the contour curves. The extreme asymmetry means gradient descent makes uneven progress.\n",
    "- **Right**: After normalization - the cost contours are much more circular and symmetric. This allows gradient descent to make balanced progress on all parameters simultaneously.\n",
    "\n",
    "This geometric insight explains why normalized features converge so much faster!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt_equal_scale(X_train, X_norm, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Summary and Key Takeaways\n",
    "\n",
    "In this lab, you've learned essential techniques for training multi-feature regression models:\n",
    "\n",
    "1. **Learning Rate Selection**: Experimented with different $\\alpha$ values and saw how:\n",
    "   - Too large → divergence\n",
    "   - Too small → slow convergence\n",
    "   - Just right → efficient training\n",
    "\n",
    "2. **Feature Scaling Power**: Discovered that z-score normalization:\n",
    "   - Enables much larger learning rates (0.1 vs 1e-7)\n",
    "   - Accelerates convergence by orders of magnitude\n",
    "   - Creates balanced parameter updates\n",
    "\n",
    "3. **Practical Implementation**: Built working code that normalizes features and makes accurate predictions\n",
    "\n",
    "These techniques are fundamental to training effective machine learning models!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Attribution\n",
    "\n",
    "The housing dataset used in this lab is derived from the [Ames Housing dataset](http://jse.amstat.org/v19n3/decock.pdf) compiled by Dean De Cock for educational purposes in data science."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc-autonumbering": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
